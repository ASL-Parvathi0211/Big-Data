# Distributed File System
    A file system is a part of operating system which stores and organise data on a secondary storage device in an effective way for fast retrival of data
    It specifies how files are to be named and where they are placed logicaly for storage and retreval
    The type of file system depends on the OS used by the computer system
    # Major file systems include the folowing
      1. FAT [File allocation Table]
          --> File allocation Table is supported by microsoft windows OS
          --> It is simple and reliable
      2. GFS [Global File System]
          --> Global File System is a file system for the LINUX, OS and it is a shared disk file system
      3. HFS [Hierarchical File System]
          --> Hierarchical File System was developed for use with MAC OS
          --> It is also used in CD ROM'secondary
      4. NTFS [New Technology File System]
          --> The NT file system also known as the New Technology file system is the default file system for windows products from windows NT 3.1 OS onwards
          --> This file system provides improvements over FAT file system
      5. UDF [Universal Disk Format]
          --> Universal Disk formt is a vendor Neutral file system used on optical media and DVD's
          --> It is the official file system for DVD video and audio as choosen by the DVD forum

# Distributed File System
  A DFS is a file system which stores data on a server
  The Data is accessed and processed as if it was stored on the local claint machine
  The DFS makes it convinient to share information and files amoung users on a network in a controlled and authorised way
  The server allows the client users to share files and store data just like they are storing the information locally
  When a user access a file on the server the server sends the user a copy of file which is catched on the users computer
  DFS is used to make files distributed across multiple servers but appear to users as if they reside in one place on the network

  # Benefits of DFS
    The advantage of using distibuted file system are
    1. Better resource management: That is users access all resource through a local system
    2. Easy accessability: Users can easily access folder at remote place without knowing the physical location of the folder
    3. Fault tolerance: That is stored data is replicated to avoid loss of data due to failure of one server
    4. Better workload management: DFS allows administraters to distribute shared folders and work loads several servers for more efficent network and server resource usage and many others

# BigData and its Importance
  Data is a group of characters or symbols on which operations are performed by a computer
  In stores and transmitted in the form of electrical signals and recorded on magnetic tape optical or mechanical recording media
  BigData is a term used to describe collection of data that is huge in size and is growing exponentially within short period of transmitted
  Common examples of BigDaa are as follows
    1. The NewYork stock exchange which generates above one tera byte of new trade data per day
    2. Social media site facebook adds 500+ tera bytes of new data everyday in terms of photo and video uploads, message exchanges, putting comments etc,.
    3. Amazon hadles 15 million customer click stream user data per day for recommended products
    4. youtube users upload 48 hours of new videos every minute of the day

# Catogeries of BigData
  BigData could be found in three forms. they are
  1. Structured
    --> Any data that can be stored, accessed, processed in the fixed format is termed s the structured data
    --> An employee table in a database is an example of structured data 
  2. Unstructured Data
    --> Any data with unknown form or structure is termed as a unstructured data
    --> An output result of google search is an example of unstructured data
  3. Semi Structured
    --> Semi structured data can contain both the forms of data
    --> Semi structured data is structured in form but it is actually not defined as a table definition in relational DBMS
    --> Example of semi structured data is a personal data of an employee stored in XML files

# Four V's in BigData or Characteristics of BigData
Four V's are the characteristics that differentiate BigData and Data
  1. Volume
    --> a particular data can actually be considered as a Big Data or not depend upon volume of data
        Ex:- A text file is a few "KB", a sound file is a few "MB", full length movie is a few "GB"
    --> Hence volume is a characteristic which needs to be considered while dealing with Big Data 

  2. velocity
    --> A data is called a Big Data when the speed of generation of data is fast
    --> Velocity measures how fast data is produced and modified
       Ex:- Youtube videos [Uploading & downloadings]
    --> Hence velocity is another characteristic which needs to be considered while depending with BigData
    
  3. variety
    --> A data is called a BigData when it is collected from heterogeneous sources and the nature of data can be both structured and unstructured
    --> In earlier days data is in the form of spread sheets and data bases
    --> But now a days data is in the form of Emails, Photos, Videos and Pdf's etc,.
    --> Hence variety is an another characteristic which needsto be considered while dealing with BigData

  4. Variability
    --> this differs to the inconsistency of data i.e., data should not be biased[format]
    --> This is where we need need to be able to identify the relevance of data and ensure data clensie is done to only store valuable data

# Drivers of BigData
While most definitions of BigData focus on the new forms of unstructured data flowing through business with new levels of volume, velocity, variety and variability thus BigData cn be represented as a simple equation.

# key drivers behind the BigData
  --> Opportunity to enable improves new business models
  --> Potentially for new technologies to drive competatively
  --> Data collected and stored continuously to improve the data
  --> Data is increased everywhere in many formats
  --> Traditional solutions are failed in new requirements
  --> Cost of the data systems as a percentage of IT spend the money continuously increasing
  --> Cost advantage of hardware & software is conviniently

# BigData Analytics
  --> The volume of the data that one has to deal has exploted to unimaginale level in the past code and at the same time the price of the data storage has systematically reduced
  --> private companies and research institutions capture Tera Bytes of data about their usersinteractions, business, socail media and also sensors from devices such as mobile phones and automobiles
  --> BigData analytics largely involves collecting data from different sources and it is a way that it becomes available to be consumed by analyst and finally delivered data products useful to the organisation business
  --> BigData analytics in different domines:

  --> The Process of converting large amount of unstructured raw data, reduced from different sources to a data products usefuk for organisation forms the core of BigData analytics

  1. Business Understanding
    --> This is intial phase focuses on understanding the project objectives and requirements from a business perspective, and then converting this knowledge into a data mining problem definition
    --> A primary plan is designed to achieve the objectives
  2. Data Understanding
    --> the data understanding phase starts with an initial dat collection and proceeds with activities in order to get familiar with the data, to identify data quality problems, to discover first insides into the data
  3. Data Preperation
    --> The data preperation phase covers or activities to construct the final data set from the inital raw data
    --> Data preperation tasks are likely to be performed multiple times and not in any described order
    --> Tasks include table, record and attribute selection as well as transformation and cleaning of data for modelling tasks
  4. Modelling 
    --> In these phase various modeling techniques are selected and applied to their parameters are calibrated [core related with those of standards] to optimal values
    --> There are several techniques for the same data mining problem type
    --> Some techniques have specific requirements on hte form of data
    --> Therefore, it is often required to step back to the Data preperation phase
  5. Evaluation
    --> At this stage in the project you have built a model that apperrs to have high quality from a data analysis perspective
    --> Before proceeding to final deployment of the model it is important to evaluate the model throughly and review the steps executed to construct the model to be certain it properly achieves the business objectives
    --> At the end of this phase a decision on the use of the data mining reults should be reached
  6. Deployment
    --> Creation of the model is generally not the end of the project even if the purpose of the model is to increase knowledge of the data, the knowledge gained will need to be organised and presented in a way thati s useful to the customer
    --> Depending upon the requirements the deployment phase can be as simple as generating a report or as complex as implementing a repeatable data scoring or data mining process

# Types of analysis or analytics
There are 4 types of analytics
  1. Diagnostic analysis
    --> It determines what has happened in the past and why it can be used to access the no.of posts, likes and reviews in a social media marketing company
  2. Predective analysis
    --> An analysis of data than can be used to predect what situation or no.of situations may result
  3. Prescriptive analysis
    --> It does not only say what is going on, but also what might happened and most importantly what to do about
  4. Descriptive analysis
    --> It uses adjusting information from the past to underastand decision in present and helps decide on effective source of action in future

# categories
  --> When 
  1. Basic Analytics
    --> It involves visualisation of simple statics
    --> This type of analytics is used especially when a lot of disperate data needs to be analysed
    --> the process of basic analytics includes investigating what happened, when it happened and its impact
  2. Advanced analytics
    --> It performs complex analysis of both structure and unstructured data by using algorithms
    --> It also used data mining techniques, machine learning, test analysis
  3. Operational analytics
    --> It means making analytics an impotant part of business process for instance, an insurance can use this model to predict the probability of client been faraudulent
  4. Monetized analytics
    --> It helps business to take important and better decision and earn revenues
    --> Credit providers it can use this data to offer value added production and technical communication company

# Applications of BigData
  --> BigData has found many application in various fields today
  --> The major fields where bigdata is being used as follows
  1. Smarter Healthcare
    --> Making use of the peta-bytes of patients data the organisation can extract measuring full information and tehn build applications that can predict the patients condition in advance
  2. Telecom
    --> Telecom sector collects infroamtion, analysis it and provides solutions to different problems
    --> By using bigdata applications telecom companies have been able to significantly reduce data packet loss, which networks are over loadded  and this providing a seemless connection to thier customers
  3. Retail 
    --> Retail is one of the greatest benefits of bigdata 
    --> The using of bigdata in retail is to understand consumer behaviour 
    --> Amazon recommondation engine provides suggestion based on the browsing history of the customers
  4. Traffic control 
    --> Traffic control is a major change for many cities 
    --> globally effective use of data and concesors will be kwy to managing
    --> Traffic better as cities become increasingly densly populated
  5. Manufacturing
    --> Analysing data in the manufacturing industry product quality, increase efficiency and save time and money
  6. Search quality
    --> Everytime we are extracting information from google 
    --> We are simultaneously generating data for it 
    --> Google stores this data and uses it to improve its search quality

# Explain about map reduce algorithm
  --> Map reduce is a high level progemming model for processing large data sets in parallel 
  --> It was originally developed by google. adopted from functional programming
  --> The model is suitable for a rnage of problems such as matrix operations, statistical frequency counting etc,.
  --> The model mapreduce algorithm contains two important tasks namely map, reduce
  *The map task takes a set of data and converts it into another set of data, where individual elements are broken down into tuples
  *The reduce task takes the output from the map as an input and combines those data tuples into a smaller set of tuples

  -->The map task is done by means of mapper class
  --> the reduce task is done by means of reduce class
  --> Map reduce implements various mathematical algorithms to devide a task into small parts and assign them to multiple systems
  --> these mathematical algorithms may include the following
  1. Sorting
    --> Sorting is one of hte basic map reduce algorithm to process and analyse data
    --> Map reduce implements sorting algotithm. It automitally sorts the data
    --> Sorting method are implemented in the mapper class itself
    --> In the shuffle and sort phase after tokanising the values in hte mapper class by user defined class collects the matching valued is as a collection
    --> To collect similar key value pairs [intermediate keys] the mapper class takes the help of row comparater class to sort the key value pairs
    --> The set of intermediate key value phase for a given reducer is automatically sorted to form key values before they are presented to the reducer
  2. Searching
    --> searching plays an important role in map reduce algorithm
    --> It helps in the combiner phase and in the reducer phase
    --> Let us try to understand how searching bugs with the help of an example
    --> The following example shows how map reduce employees searching algorithm to find out the details of the employee to draws the heighest salary in a given employee data set
    1. Let us assume we have employee data in four different files a, b, c, d 
    2. Let us also assume there are duplicate employee records in all four files because of importing of employee data from all database tables repeatedly
    --> The "map phase" process each input file and provides the employee data in key_value pairs (<k,v>:<employee name, salary>) 
    --> The combiner phase(searching technique) will accept the input from the map phase as a key_value pairs with emp name and salary
    --> Using searching technique the combinor will check all the employee salary to find the highest salaried employee in each file 
      <k:employee name, v:salary>
      max = The salary of an first employee treated as max salary 
      if (v(second employee)salary>max)
      {
        max = v(salary);
      }
      else 
      {
      continue checking
      }
    --> The expected result is as follows
    <Varsha, 30000> <Surya, 40000> <Naga, 22000> <ANIL, 25000> 

# Reducer phase
  --> It from each file you will find the highest salaried employee
  --> To avoid redundency check all the <k,v> and eliminate duplicate entries if any
  --> The same algorithm is used in between the four <k,v> which are coming from four input files
  --> The final output should be as follows: <surya, 4000>

Matrix_Vector multiplication by map reduce
  --> Map reduce is a high level programming model for processing large data sets in parallel
  --> It was originally developed by google, adopted from functional programming
  --> The model is suitable for a range of problems such as matrix operations, statistical frequency counting etc,. 
  Matrix Vector Application 
    --> There are different implementations of matrix vector multiplication depending on the vector fits into the main memory or not 
    --> This example demonistrates a basic version of matrix vector multiplication in which the vector fits into the main memory

    --> We store the spare matrix as a triple with explicet co-ordinate (i, j, aij)
    --> the value of the first entry of hte matrix [0,0] is 3
    --> Similarly the value of the second entry in (1,0) is 2
    --> We donot store the zero entries of hte matrix
    --> This is the spare matrix representation of hte matrix of the matrix from the figure above

    --> We store the vector in a dense format without explicit co-ordinate
    --> One of the map function takes the first element of the matrix(0,0,3) be multiplies it with the first element of hte vector '4'and produce an intermediate key_value pairs(0, 12)
    --> The key '0' that is the row position of hte element in the spare matrix, associates the value(12) with its position in the matrix vector product 
    --> Another map function takes the second element (1, 0, 2) multiplies it with the second row of the vector '3' producing an intermediate key value pair (0, 6) etc,.
    --> Reduce function performs a summary operation on the intermediate keys
    --> An intermediate key value with the key '0' will be summed up under a final index '0'


      