# Hadoop
  --> Hadoop is a frame work that allows you to first store big data in a distributed environment so that you can process it parallely
  --> There are basically two components in Hadoop

  --> The first one in HDFS (Hadoop distributed file system) for storage, that allows you to store data of various formats across a cluster
  --> The second one is YARN for resource management in Hadoop
  --> It allows parallel processing over the data i.e., stored across HDFS
    a. HDFS Architecture
      --> In HDFS name node is the master node and data nodes are the slaves
      --> HDFS is the primary storage unit in the Hadoop
      --> The HDFS is the reason behind the quick data accessing and scalability of Hadoop
      --> The HDFS compresses the following components
        1. name node
          --> It is the master node that maintains and manages the datanode
          --> It records the meta data of all the blocks stored in the cluster 
           EX:- Location of bloack stored, size of the files, permission etc,.
          --> It records each and every change that take place to the file system meta data
          --> If a file is deleted in HDFS the name node will immediately record this in the edit log
          --> It regularly receives a heartbeat and a block report from all the data nodes are live 
          --> It keeps a record of all the blocks in the HDFS and data nodes in which they are stored
        2. Data node 
          --> It is the slave node which run on each slave machine
          --> The actual data is stored in database
          --> It is responsible for serving read/write request from the clients
          --> It is also responsible for creating blocks, deleting blocks and replicating the same based on the decisions taken by the name node 
        3. Secondary name node
          --> As the name speak the secondary name node is not a backup of the name node
          --> It acts as a buffer to the name node
          --> It stores the intermediate updates of the name node and updates the information when the name node is inactive
    b. YARN [Yet another resource negotiator]
      --> It provides the resource management
      --> YARN is called as the operating system of Hadoop
      --> As it is responsible for managing and monitoring work loads
      --> An YARN compress of the following components:
        1. Resource Manager
          --> It is the core component of yarn and it is considered as the "Master"
          --> It is responsible for providing generic and flexible frame works to administrate the computing resource in a hadoop cluster
          --> Resource manager receives the processing request and passes the parts of request to corresponding node managers
          --> It keeps a track of the heart beats from the node manager
        2. Node Manager
          --> It is the slave and it serves the resource manager
          --> Node manager is assigned to all the nodes in a cluster
          --> The main responsibility of the node manager is to monitor the status of the container and app managers
          --> Node managers installed on every data nodes
          --> It is responsible for execution of task on every single data node

# Hadoop Eco systems
  --> Hadoop ecosystems is a platform or framework which provides various service to solve the Bigdata problems
  --> Hadoop can be used for many things it is defined in many ways
  --> They are 
    1. It is a data management system bringing together large amounts of structured and unstructured data 
    2. It is a parallel execusion frameworks bringing the power of super commuting
    3. It is an open source community creating tools and software for soluting bigdata problems 
  --> Hadoop is an ecosystem compressed of amny compenents that range from data storage, to data integration, to data processing, to specialise tools for data analysis 
  --> the four components of hadoop ecosystem are shown below
    1. HDFS 
      --> A fundational component of the hadoop ecosystem is the HDFS 
      --> HDFS is the mechanism by which a large amount of data can be distributed over a cluster of computers, and data is written once but read many times for analytics 
      --> It provides the foundation for other tools such as Hbase 
    2. Map reduce 
      --> Hadoop main execution framework is map reduce, a programming model for distributed, parallel data processing, breaking jobs into mapping phase and reduce phase 
      --> Developers write map reduce jobs for hadoop using data stored in HDFS for fast data accessing 
      --> Map reduce make it possible to carry over the processing logic and helps to write applications to transform big data sets into a managable one 
    3. Pig [ Scripting ]
      --> Pig was basically developed by yahoo which works on a pig latin language, which is query based language similar to SQL
      --> In pig first the load commands, loads the data, then we perform various functions on it like grouping, Filtering, Joining, Sorting etc,. 
      --> Atlast either you can dump the data on the screen or you can store the result back in HDFS
      --> 10 lines of pig latin = approximately 200 lines of map reduce java code
    4. HBase
      --> It stores data in HDFS
      --> It is a noSQL database or non relational database
      --> It mainly used when you need random, real time, read/write access to your bigdata
      --> It supports high volume of data and high through put 
      --> In HBase table can have thousands of columns
    5. Zoo Keeper [ Centralized service for manafacturing configuration information ] 
      --> Zoo keeper is hadoop distributed co-ordinate service
      --> It designed to run over a cluster of machines, it is a highly available service used for the management of hadoop operations, and many components of hadoop depend on it  
    6. Oozie [ Workflow ]
      --> Oozie is a work flow or co-ordination system used to manage the hadoop jobs
      --> There are two kinds of jobs
      --> That is oozie workflow and oozie co-ordinate jobs
      --> It is capable of managing events that include timing and presence of required data
    7. Hive [ SQL Query ]
      --> An SQL like high level langage used to run queries on data stored in hadoop
      --> Hive enables developers not femiliar with map reduce to write data queries that are translated into map reduce jobs in hadoop 
      --> Like pig, hive was developed as an obstraction layer, but generate more towards database analyst more familiar with SQL than java programming 
    8. Sqoop [ data Collection ]
      --> It is a tool to design to transfared data between hadoop and relational database servers 
      --> It is used to import data from relational database such as ORACLE and mySQL to HDFS and export data from HDFS to relational database 
    9. Flume [ Data Collection ]
      --> Flume is a distributed, reliable and highly available service for efficiently collecting, aggregating and moving large amounts of data from individual machines to HDFS 

# Moving data in and out of hadoop
  --> Moving large quantities of data in and out of hadoop offers many challenges
  --> THe key elements that are to be considered when working with data moments are
    1. Idempotence 
      --> An idempotent operation produce the same result no matter how many times its executed
      --> In a relational database the inserts are not idempotent
      --> Because executing them multiple times does not produce the same result in database started
      --> Alternatively updates often are idempotent because they will produce the same end result 
      --> Any time data is being written idempotency should be a consideration in hadoop
    2. Aggregation 
      --> The data aggregation process combines multiple data elements
      --> In the context of data ingrease, these can be useful because moving large quantities of small files into HDFS potentially translates into name node memory as well as slow map reduce reduce execution times 
    3. Data Format Transformation 
      --> The data transformation process converts one data format into another
      --> The source data is not in a format i.e., ideal for processing in tools such as map reduce
      --> IF the source data is in multi-line XML form and want to apply a pre-processing step
      --> This would converts the data into a form that can be split such as XML element per line or converts it into a format such as "AVRO"
    4. compression
      --> Compression not only helps by reducing the foot print of data at rest, but also has IO advantages when reading and writing data
    5. Availability and Recoverability
      --> Recoverability allows an ingrease or Egress tool to retry in the event of a failed operation
      --> Because its unlikely that any data source or hadoop itself can be 100% available, its important that an ingress or agress action be retreived retried in the event of failure 
    6. Reliable data transfer & Data Validation
      --> In the context of data transformation checking for correctness is how you verify that no data corruption occured as the data was in transit 
      --> A common method for checking for checking the correctness of raw data such as storage devices is cyclic reduntancy check [ CRC ] which are what HDFS uses internally too maintain block level integrity 
    7. Resource Consumption & Performance
      --> Resource consumption and performance are measures of system resource utilization and system efficiency
      --> For performance whether the tool performs ingress or agress activities in parallel and if so what mechanism it provides to tune the amount of parallalism
    8. Monitoring 
      --> Monitoring ensures that functions are performing as expected in automated systems
      --> Monitoring should also includes verifying that the data volumes being moved are at expected level
    9. Spectiative Execution 
      --> Map reduce has a feature called spectilative execution that launches duplicates tasks near the end of the job for tasks that are stiff executing
      --> This helps prevent slow hardware from impacting job execution time
      --> But if a map task is used to perform inserts into a relational database there can be parallel processes inserting the same data 

# Understanding inputs and outputs of map reduce
    a.Understanding inputs of map reduce
      --> The two classes but that support data input in map reduce are input format and record recorder
        1. Input Format
          --> The input format class is used to determine how the input data should be partitioned for the map task
          --> Every job in map reduce must definr its inputs according to the input-format abstract class
          --> Input format describe type information for map input keys and values, they specify how the input data should be partitioned and they indicate the record reader instance that should read the data from source 
        2. Record Reader
          --> Record reader performs the reading of data from the inputs
          --> Record reader calss in the map task is used to read data from an input
          --> A task is commonly created for each input split and each task has a single record reader thats responsible for reading the data for that input split
    b. Understanding outputs of map reduce
      --> The two classes that support data output in map reduce are output format and a record writter
        1. Output Format
          --> The output foramt performs some basic validation of the data sink properties 
          --> The output format class should check the information related to the job output, providing a record writter, and specyfing and output commiter which allows to be staged and then made perminent upon tasks or job success
        2. Record writter
          --> The record writter class is used to write the reducer outputs to the determined data link

# Data Serialization
  -->Serialization is the process of converting structured object, data into a byte stream
  --> It is done basically for two purpose
  --> One for transmission over a network [ inter process communication ] and the second one for writing to persistent storage
  --> In hadoop the inter process communication between nodes in the system is done by using remote procedure call [RPC]
  --> The RPC protocal users sertalization to make the message into a binary stream to be sent to the remote node 
  --> The remote node receives and deserializers the binary stream into the original message
  --> The RPC serialization format is required to be as follows:
    1. Compact
      --> To make the best use of network band with, which is the most resource in a data Centralized
    2. fast
      --> Since is the communication between the nodes is srucial in distributed systems the serialization and deserialization process should be quick, producing less over happened
  
    Extensible
      --> To adopt to new changes and requirements
    Interoperable
      --> The message format should support the nodes that are written it in different languages 
      --> Hadoop uses its own serialization format i.e, writables
      --> Writable is compact and fast but not extensable or Interoperable
    Persistent Storage 
      --> Persistent storage is a digital storage facility that does not lose its data with the loss of power supply. For example, Mangnetic disks and Hard disk Drivers 

    # Serializing the Data in Hadoop
      --> The process to serialize the integer type of data in discussed below
      *Instantiate IntWritable class by wrapping an integer value in it
      *Instantiate ByteArray Output Stream class
      *Instantiate DataOutputStream class and pass the object of ByteArrayOutputStream class to it
      *Serialize the integer value in IntWritable object using write() method. This method needs an object of DataOutputStream class 
      *The serialized data will be stored in the byte array object which is passed as parameter to the DataOutputStream class at the time of installation. Convert the data in the object to byte array 
        Writable Interface
          This is the interface in hadoop which provides methods for serialization and deserialization. The following table describes the methods
          *Void read fields (Data Input in): This method is used to deserialize the fields of the given object
          *void write (DataOutput out): This method is used to serialize the fields of the given object 
        InWritable class
          This class implements Writable, Comparable and Writable Comparable interfaces. This class provides method used to serialize and deserialize integer type of data
          *Int Writable(), Int Writable(int value)(Classes): int get   using this method you can get the integer value present in the current object
          *Vod Set(int value): This method is used to set the value of the current Intwritable object
